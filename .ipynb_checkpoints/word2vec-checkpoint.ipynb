{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Word2Vec\n",
    "#### The model Word2Vec is a simple word embedding neural network with a single hidden layer, based on the study of Le & Mikolov (2014)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings from plain text.\n",
    "\n",
    "The model assumes the *Distributional Hypothesis* that words are characterized by words they hang out with. this idea is used to estimate the probability of two words occurring near each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is a very simple neural network with a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = ['the king loves the queen', 'the queen loves the king',\n",
    "             'the dwarf hates the king', 'the queen hates the dwarf',\n",
    "             'the dwarf poisons the king', 'the dwarf poisons the queen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text2sentences(text):\n",
    "    return text.lower().split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class Word2Vec(object):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        self.N = 3 # number of nodes in Hidden Layer\n",
    "        self.V = None # Vocabulary size\n",
    "        self.WI = None\n",
    "        self.WO = None\n",
    "        self.vocabulary = None\n",
    "        self.learning_rate = 1.0\n",
    "    \n",
    "    def Vocabulary(self):\n",
    "        dictionary = defaultdict()\n",
    "        # len of dictionary gives a unique integer to each new word\n",
    "        dictionary.default_factory = lambda: len(dictionary) \n",
    "        return dictionary\n",
    "\n",
    "    def docs2bow(self, docs, dictionary):\n",
    "        \"\"\"Transforms a list of strings into a list of lists where \n",
    "        each unique item is converted into a unique integer.\"\"\"\n",
    "        for doc in docs:\n",
    "            yield [dictionary[word] for word in doc.split()] # returns a generator\n",
    "    \n",
    "    def sentences2bow(self):\n",
    "        self.vocabulary = self.Vocabulary()\n",
    "        bow = list(self.docs2bow(self.sentences, self.vocabulary))\n",
    "        return bow\n",
    "    \n",
    "    def random_init(self):\n",
    "        self.V = len(self.vocabulary)\n",
    "        \n",
    "        # random initialization of weights between [-0.5 , 0.5] normalized by number of nodes mapping to.\n",
    "        self.WI =(np.random.random((self.V, self.N)) - 0.5) / self.N # input weights\n",
    "        self.WO =(np.random.random((self.N, self.V)) - 0.5) / self.V # output weights\n",
    "        \n",
    "    def context_prob(self, context):\n",
    "        return sum(np.exp(self.WI[self.vocabulary[context]].dot(self.WO.T[self.vocabulary[word]]))\n",
    "                   for word in self.vocabulary)\n",
    "    \n",
    "    def backprop(self, context, target):\n",
    "        denominator = self.context_prob(context)\n",
    "\n",
    "        for word in self.vocabulary:\n",
    "\n",
    "            numerator = np.exp(self.WI[self.vocabulary[context]].dot(self.WO.T[self.vocabulary[word]]))\n",
    "            P_word_context = numerator / denominator # posterior probability P(word | context)\n",
    "\n",
    "            if word == target:\n",
    "                t = 1\n",
    "                print \"P(target|context)\", P_word_context\n",
    "            else:\n",
    "                t = 0\n",
    "\n",
    "            err = t - P_word_context # error\n",
    "            #print \"Error: \", err\n",
    "\n",
    "            # weight update using stochastic gradient descent\n",
    "            self.WO.T[self.vocabulary[word]] -= self.learning_rate * err * self.WI[self.vocabulary[context]]\n",
    "            # update brings word vector closer in the feature space if word = target, and push them apart otherwise.\n",
    "\n",
    "        self.WI[self.vocabulary[context]] -= self.WO.sum(axis=1) # update only weights for input word\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        bow = self.sentences2bow()\n",
    "        # visualize bag-of-word sentence conversion\n",
    "        # print bow\n",
    "        \n",
    "        self.random_init()\n",
    "        \n",
    "        for sentence in self.sentences:\n",
    "            prev_word = None\n",
    "            for word in sentence.split():\n",
    "                if prev_word != None:\n",
    "                    target = word\n",
    "                    context = prev_word\n",
    "                    self.backprop(context, target)\n",
    "                prev_word = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(target|context) 0.143473967631\n",
      "P(target|context) 0.143059248075\n",
      "P(target|context) 0.142646438945\n",
      "P(target|context) 0.14364795132\n",
      "P(target|context) 0.135354000989\n",
      "P(target|context) 0.142306570948\n",
      "P(target|context) 0.135610380013\n",
      "P(target|context) 0.162206459026\n",
      "P(target|context) 0.162699267825\n",
      "P(target|context) 0.142914135961\n",
      "P(target|context) 0.148657993322\n",
      "P(target|context) 0.106086118779\n",
      "P(target|context) 0.0912081448296\n",
      "P(target|context) 0.181075248948\n",
      "P(target|context) 0.14602407654\n",
      "P(target|context) 0.0454139659483\n",
      "P(target|context) 0.00154787392611\n",
      "P(target|context) 0.186976530737\n",
      "P(target|context) 0.129804793258\n",
      "P(target|context) 0.000872148893805\n",
      "P(target|context) 1.33377106512e-07\n",
      "P(target|context) 0.126644109941\n",
      "P(target|context) 0.140128027403\n",
      "P(target|context) 2.18473559499e-08\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-581effab6aa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msentences_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentences_bow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "vocabulary = Vocabulary()\n",
    "sentences_bow = list(docs2bow(sentences, vocabulary))\n",
    "sentences_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b54bd45daa73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = len(vocabulary)\n",
    "N = 3 # number of nodes in Hidden Layer\n",
    "\n",
    "# random initialization of weights between [-0.5 , 0.5] normalized by number of nodes mapping to.\n",
    "WI =(np.random.random((V, N)) - 0.5) / N # input weights\n",
    "WO =(np.random.random((N, V)) - 0.5) / V # output weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print WI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print WO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# input weights associated with dwarf\n",
    "WI[vocabulary['dwarf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output weights associated with hates\n",
    "WO.T[vocabulary['hates']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dot product $W_I \\cdot W'^T_O$ we compute the distance between the input word *dwarf* and the output word *hates*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WI[vocabulary['dwarf']].dot(WO.T[vocabulary['hates']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using softmax regression, we can compute the posterior probability $P(w_O|w_I)$:\n",
    "\n",
    "$$ P(w_O|w_I) = y_i = \\frac{exp(W_I \\cdot W'^T_O)}{\\sum^V_{j=1} exp(W_I \\cdot W'^T_j)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numerator = np.exp(WI[vocabulary['dwarf']].dot(WO.T[vocabulary['hates']]))\n",
    "denominator = sum(np.exp(WI[vocabulary['dwarf']].dot(WO.T[vocabulary[word]])) for word in vocabulary)\n",
    "\n",
    "P_hates_dwarf = numerator / denominator\n",
    "P_hates_dwarf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the hidden-to-output layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function to minimize is: $E = -\\log P(w_O|w_I)$\n",
    "\n",
    "The error is computed with $t_j - P(w_O|w_I) = e_j$, where $t_j$ is 1 if $w_j$ is the actual output word, otherwise $t_j$ is 0.\n",
    "\n",
    "To obtain the gradient on the hidden-to-output weights, we compute $e_j \\cdot h_i$, where $h_i$ is a copy of the vector corresponding to the input word (only holds with a context of a single word). Finally, using stochastic gradient descent, with a learning rate $\\nu$ we obtain the weight update equation for the hidden to output layer weights:\n",
    "\n",
    "$$W'^{T (t)}_j = W'^{T (t-1)}_j - \\nu \\cdot e_j \\cdot h_j$$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_word = 'king' \n",
    "input_word = 'queen' # context word\n",
    "learning_rate = 1.0\n",
    "\n",
    "denominator = sum(np.exp(WI[vocabulary[input_word]].dot(WO.T[vocabulary[word]])) for word in vocabulary)\n",
    "\n",
    "for word in vocabulary:\n",
    "    \n",
    "    numerator = np.exp(WI[vocabulary[input_word]].dot(WO.T[vocabulary[word]]))\n",
    "    P_word_queen = numerator / denominator # posterior probability P(word | queen)\n",
    "    \n",
    "    if word == target_word:\n",
    "        t = 1\n",
    "    else:\n",
    "        t = 0\n",
    "    \n",
    "    err = t - P_word_queen # error\n",
    "    \n",
    "    # weight update using stochastic gradient descent\n",
    "    WO.T[vocabulary[word]] -= learning_rate * err * WI[vocabulary[input_word]]\n",
    "    # update brings word vector closer in the feature space if word = target, and push them apart otherwise.\n",
    "    \n",
    "print WO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the input-to-hidden layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backpropagate the prediction errors to the input-to-hidden weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WI[vocabulary[input_word]] -= WO.sum(axis=1) # update only weights for input word\n",
    "WI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "denominator = sum(np.exp(WI[vocabulary[input_word]].dot(WO.T[vocabulary[word]])) for word in vocabulary)\n",
    "\n",
    "for word in vocabulary:\n",
    "    numerator = np.exp(WI[vocabulary[input_word]].dot(WO.T[vocabulary[word]]))\n",
    "    P_word_queen = numerator / denominator # posterior probability P(word | queen)\n",
    "    \n",
    "    print word, P_word_queen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-word context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_word = 'king'\n",
    "context = ['queen', 'loves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = (WI[vocabulary[context[0]]] + WI[vocabulary[context[1]]]) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "denominator = sum(np.exp(h.dot(WO.T[vocabulary[word]])) for word in vocabulary)\n",
    "\n",
    "for word in vocabulary:\n",
    "    \n",
    "    numerator = np.exp(h.dot(WO.T[vocabulary[word]]))\n",
    "    P_word_context = numerator / denominator # posterior probability P(word | context)\n",
    "    \n",
    "    if word == target_word:\n",
    "        t = 1\n",
    "    else:\n",
    "        t = 0\n",
    "    \n",
    "    err = t - P_word_context # error\n",
    "    \n",
    "    # weight update using stochastic gradient descent\n",
    "    WO.T[vocabulary[word]] -= learning_rate * err * h\n",
    "    # update brings word vector closer in the feature space if word = target, and push them apart otherwise.\n",
    "    \n",
    "print WO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for input_word in context:\n",
    "    WI[vocabulary[input_word]] -= (1. / len(context)) * learning_rate * WO.sum(axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = (WI[vocabulary[context[0]]] + WI[vocabulary[context[1]]]) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "denominator = sum(np.exp(h.dot(WO.T[vocabulary[word]])) for word in vocabulary)\n",
    "\n",
    "for word in vocabulary:\n",
    "    numerator = np.exp(h.dot(WO.T[vocabulary[word]]))\n",
    "    P_word_context = numerator / denominator # posterior probability P(word | context)\n",
    "    \n",
    "    print word, P_word_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = len(vocabulary)\n",
    "N = 3 # number of hiden nodes\n",
    "P = 5 # number of sentences\n",
    "\n",
    "WI = (np.random.random((V, N)) - 0.5) / N\n",
    "WO = (np.random.random((N, V)) - 0.5) / V\n",
    "D =  (np.random.random((P, N)) - 0.5) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = ['snowboarding is dangerous', 'skydiving is dangerous',\n",
    "             'escargots are tasty to some people', 'everyone loves tasty food',\n",
    "             'the minister has some dangerous ideas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = Vocabulary()\n",
    "sentences_bow = list(docs2bow(sentences, vocabulary))\n",
    "sentences_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_word = 'dangerous'\n",
    "h = (D[0] + WI[vocabulary['snowboarding']]) /2.0\n",
    "learning_rate = 1.0\n",
    "\n",
    "denominator = sum(np.exp(h.dot(WO.T[vocabulary[word]])) for word in vocabulary)\n",
    "\n",
    "for word in vocabulary:\n",
    "    \n",
    "    numerator = np.exp(h.dot(WO.T[vocabulary[word]]))\n",
    "    P = numerator / denominator # posterior probability P(word | context, phrase)\n",
    "    \n",
    "    if word == target_word:\n",
    "        t = 1\n",
    "    else:\n",
    "        t = 0\n",
    "    \n",
    "    err = t - P # error\n",
    "    \n",
    "    # weight update using stochastic gradient descent\n",
    "    WO.T[vocabulary[word]] -= learning_rate * err * h\n",
    "    # update brings word vector closer in the feature space if word = target, and push them apart otherwise.\n",
    "    \n",
    "print WO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EH = WO.sum(axis=1)\n",
    "len_context = 2\n",
    "WI[vocabulary['snowboarding']] -= (1. / len_context) * learning_rate * EH\n",
    "D[0] -= (1. / len_context) * learning_rate * EH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
