{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Models for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Word2Vec model is a simple word embedding neural network, developed by Mikolov et al. (2013).\n",
    "####  *Such continuous word embedding representations have have been proven to be able to carry semantic meanings and are useful in various NLP tasks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have attempted to implement three language models described in *Le & Mikolov (2014)*'s paper ** *Distributed Representations of Sentences and Documents* **.\n",
    "\n",
    "The implementations don't make use of any NLP libraries and consist of the simplest form of the algorithm without any optimization.\n",
    "The aim of this notebook is simply to gain:\n",
    "* understanding of the **language models' algorithm**\n",
    "* intuition on **word embedding representations**\n",
    "* understanding of inner workings of **neural networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, popular fixed-length features are **bag-of-words**.\n",
    "\n",
    "However bag-of-words features have two major weaknesses: \n",
    "- they lose the **ordering** of the words.\n",
    "- they also ignore **semantics** of the words. \n",
    "\n",
    "In the paper by **Le & Mikolov (2014)**, they propose a distributed representations of sentences and documents, which they call ** *Paragraph Vector* **. \n",
    "\n",
    "The model assumes the ** *Distributional Hypothesis* ** that words are characterized by other words in their vicinity. This idea is used to estimate the probability of two words occurring near each other.\n",
    "\n",
    "*Paragraph Vector* is an unsupervised algorithm that learns fixed-length feature representations from **variable-length pieces of texts**, such as sentences, paragraphs, and documents. The algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives the algorithm the potential to overcome the **weaknesses** of bag-of- words models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/3.png\"  width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical results, in the paper, show that **Paragraph Vectors outperform bag-of-words models**, as well as, other techniques for text representations. For example, **word weighting functions** (which requires task-specific tuning) and **parse trees**. The authors were able to achieve new **state-of-the-art** results on several **text classification** and **sentiment analysis** tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after training words with similar meaning are mapped to a similar position in the vector space. The difference between word vectors also carry meaning. For ex-ample, the word vectors can be used to answer analogy questions using simple vector algebra: “King” - “man” + “woman” = “Queen”.\n",
    "Theses properties of word vectors are useful in many NLP task, such as language modelling and understanding and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we concatenate the paragraph vector with several word vectors from a paragraph and predict the following word in the given context. Both word vectors and paragraph vectors are trained by the stochastic gradient descent and backpropaga- tion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "After training, the word vectors are mapped into a vector space such that semantically similar words have similar vector representations.\n",
    "In the study they extend the model to go beyond word level to achieve phrase-level or sentence-level representations. \n",
    "\n",
    "\n",
    "Paragraph Vector takes a general approach. It is capable of constructing representations of input sequences of any length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Word Context Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first implementation, I looked at the simplest form of the *Continuous Bag-Of_Word Model (CBOW)* introduced in *Mikolov et al. (2013a)*. This model has just **one word** as the context to predict the next target world in the sentence. This context vector is fed into a **neural network** with a **single hidden layer** that is fully connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image/1neural-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input vector is a one-hot encoded vector. Each word is mapped into its own unique vector of features, represented by a column in a matrix W_input.\n",
    "There is also another weight matrix, W_output connecting the hidden layer to the output layer.\n",
    "Given a context word these weights matrices, we can compute a score for each word in the vocabulary.\n",
    "\n",
    "This is essentially a multiclass classification where we have as many labels as our vocabulary size V.\n",
    "soft-max, a log-linear j\n",
    "classification model, to obtain the posterior distribution of words, which is a multinomial\n",
    "distribution.\n",
    "\n",
    "maximize the average log probability\n",
    "\n",
    "[EQ]\n",
    "Now using softmax regression, we can compute the posterior probability $P(w_O|w_I)$:\n",
    "\n",
    "$$ P(w_O|w_I) = y_i = \\frac{exp(W_I \\cdot W'^T_O)}{\\sum^V_{j=1} exp(W_I \\cdot W'^T_j)} $$\n",
    "\n",
    "In the paper hierarchical softmax is used for faster training, but for simpliticy I will just use a regular softmax regression as the multiclass classifier\n",
    "\n",
    "[EQ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the hidden-to-output layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function to minimize is: $E = -\\log P(w_O|w_I)$\n",
    "\n",
    "The error is computed with $t_j - P(w_O|w_I) = e_j$, where $t_j$ is 1 if $w_j$ is the actual output word, otherwise $t_j$ is 0.\n",
    "\n",
    "To obtain the gradient on the hidden-to-output weights, we compute $e_j \\cdot h_i$, where $h_i$ is a copy of the vector corresponding to the input word (only holds with a context of a single word). Finally, using stochastic gradient descent, with a learning rate $\\nu$ we obtain the weight update equation for the hidden to output layer weights:\n",
    "\n",
    "$$W'^{T (t)}_j = W'^{T (t-1)}_j - \\nu \\cdot e_j \\cdot h_j$$.\n",
    "\n",
    "Using the dot product $W_I \\cdot W'^T_O$ we compute the distance between the input word *dwarf* and the output word *hates*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the input-to-hidden layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backpropagate the prediction errors to the input-to-hidden weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = ['<s> the prince loves skateboarding in the park </s>', \n",
    "             '<s> the princess loves the prince but the princess hates skateboarding </s>',\n",
    "             '<s> skateboarding in the park is popular </s>',\n",
    "             '<s> the prince is popular but the prince hates attention </s>',\n",
    "             '<s> the princess loves attention but the princess hates the park </s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec_1WordContext(sentences)\n",
    "W, vocab = model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-word context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image/2neural-network-cbow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec_nWordContext(sentences, learning_rate = 1.0, context_size = 4)\n",
    "W, vocab = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~DemoAccount/4999.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.graph_vector_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec_nWordContext(sentences, learning_rate = 1.0, context_size = 2)\n",
    "W, vocab = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
