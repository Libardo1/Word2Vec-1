{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Word2Vec\n",
    "#### The model Word2Vec is a simple word embedding neural network with a single hidden layer, based on the study of Le & Mikolov (2014)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings from plain text.\n",
    "\n",
    "The model assumes the *Distributional Hypothesis* that words are characterized by words they hang out with. this idea is used to estimate the probability of two words occurring near each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP popular fixed-length features are **bag-of-words**.\n",
    "\n",
    "However bag-of-words features have two major weaknesses: \n",
    "- they lose the ordering of the words\n",
    "- they also ignore semantics of the words. \n",
    "\n",
    "In the paper by **Le & Mikolov (2014)**, they propose a distributed representations of sentences and documents, which they call *Paragraph Vector*. \n",
    "\n",
    "It's an unsupervised algorithm that learns fixed-length feature representations from **variable-length pieces of texts**, such as sentences, paragraphs, and documents. The algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives the algorithm the potential to overcome the weaknesses of bag-of- words models. \n",
    "\n",
    "Empirical results show that **Paragraph Vectors outperform bag-of-words models** as well as other techniques for text representations. The paper then shows that they were abke to achieve new **state-of-the-art** results on several **text classification** and **sentiment analysis** tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after training words with similar meaning are mapped to a similar position in the vector space. The difference between word vectors also carry meaning. For ex-ample, the word vectors can be used to answer analogy questions using simple vector algebra: “King” - “man” + “woman” = “Queen”.\n",
    "Theses properties of word vectors are useful in many NLP task, such as language modelling and understanding and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we concatenate the paragraph vector with several word vec- tors from a paragraph and predict the following word in the given context. Both word vectors and paragraph vectors are trained by the stochastic gradient descent and backpropaga- tion\n",
    "\n",
    "neural network language model proposed:\n",
    "\n",
    " - each word is represented by a one -hot  vector\n",
    " - if we use a multi-word context, then the vectors are averaged \n",
    " - this context vector is the input of a neural network, and tries to predict the next word.\n",
    "\n",
    "### Results\n",
    "\n",
    "After training, the word vectors are mapped into a vector space such that semantically similar words have similar vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the study they extend the model to go beyond word level to achieve phrase-level or sentence-level representations. \n",
    "\n",
    "Paragraph Vectors is less complex and outperforms other methods that have tried to achieve similar representations, such as **word weighting functions** (which requires task-specific tuning) and **parse trees**. \n",
    "\n",
    "Paragraph Vector takes a general approach. It is capable of constructing representations of input sequences of any length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Word Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before every word is mapped to a unique vector, represented by a column in a matrix W. It is then used as features for prediction of the next word in a sentence.\n",
    "\n",
    "maximize the average log probability\n",
    "\n",
    "[EQ]\n",
    "\n",
    "In the paper hierarchical softmax is used for faster training, but for simpliticy I will just use a regular softmax regression as the multiclass classifier\n",
    "\n",
    "[EQ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is a very simple neural network with a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = ['<s> the prince loves skateboarding in the park </s>', \n",
    "             '<s> the princess loves the prince but the princess hates skateboarding </s>',\n",
    "             '<s> skateboarding in the park is popular </s>',\n",
    "             '<s> the prince is popular but the prince hates attention </s>',\n",
    "             '<s> the princess loves attention but the princess hates the park </s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "class Word2Vec_1WordContext(object):\n",
    "    def __init__(self, sentences, learning_rate = 1.0, nodes_HL = 3):\n",
    "        self.sentences = sentences\n",
    "        self.N = nodes_HL # number of nodes in Hidden Layer\n",
    "        self.V = None # Vocabulary size\n",
    "        self.WI = None\n",
    "        self.WO = None\n",
    "        self.vocabulary = None\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def Vocabulary(self):\n",
    "        dictionary = defaultdict()\n",
    "        # len of dictionary gives a unique integer to each new word\n",
    "        dictionary.default_factory = lambda: len(dictionary) \n",
    "        return dictionary\n",
    "\n",
    "    def docs2bow(self, docs, dictionary):\n",
    "        \"\"\"Transforms a list of strings into a list of lists where \n",
    "        each unique item is converted into a unique integer.\"\"\"\n",
    "        for doc in docs:\n",
    "            yield [dictionary[word] for word in doc.split()] # returns a generator\n",
    "    \n",
    "    def sentences2bow(self):\n",
    "        self.vocabulary = self.Vocabulary()\n",
    "        bow = list(self.docs2bow(self.sentences, self.vocabulary))\n",
    "        return bow\n",
    "    \n",
    "    def random_init(self):\n",
    "        self.V = len(self.vocabulary)\n",
    "        \n",
    "        # random initialization of weights between [-0.5 , 0.5] normalized by number of nodes mapping to.\n",
    "        self.WI =(np.random.random((self.V, self.N)) - 0.5) / self.N # input weights\n",
    "        self.WO =(np.random.random((self.N, self.V)) - 0.5) / self.V # output weights\n",
    "    \n",
    "    def softmax_regression(self, word, h):\n",
    "        # posterior probability P(word | context)\n",
    "        return (np.exp(h.dot(self.WO.T[self.vocabulary[word]])) / \n",
    "                sum(np.exp(h.dot(self.WO.T[self.vocabulary[w]])) for w in self.vocabulary))\n",
    "    \n",
    "    def backprop(self, context, target):\n",
    "\n",
    "        for word in self.vocabulary:\n",
    "\n",
    "            h = self.WI[self.vocabulary[context]]\n",
    "            P_word_context = self.softmax_regression(word, h)\n",
    "            \n",
    "            if word == target:\n",
    "                t = 1\n",
    "                #print \"P(target|context)\", P_word_context\n",
    "            else:\n",
    "                t = 0\n",
    "\n",
    "            err = t - P_word_context # error\n",
    "\n",
    "            # weight update using stochastic gradient descent\n",
    "            self.WO.T[self.vocabulary[word]] -= self.learning_rate * err * h\n",
    "            # update brings word vector closer in the feature space if word = target, and push them apart otherwise.\n",
    "\n",
    "        self.WI[self.vocabulary[context]] -= self.learning_rate * self.WO.sum(axis=1) # update only weights for input word\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        bow = self.sentences2bow()\n",
    "        # visualize bag-of-word sentence conversion\n",
    "        # print bow\n",
    "        \n",
    "        self.random_init()\n",
    "        \n",
    "        for sentence in self.sentences:\n",
    "            prev_word = None\n",
    "            for word in sentence.split():\n",
    "                if prev_word != None:\n",
    "                    target = word\n",
    "                    context = prev_word\n",
    "                    self.backprop(context, target)\n",
    "                prev_word = word\n",
    "        return self.WI, OrderedDict(sorted(self.vocabulary.items(), key=lambda t: t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec_1WordContext(sentences)\n",
    "W, vocab = model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dot product $W_I \\cdot W'^T_O$ we compute the distance between the input word *dwarf* and the output word *hates*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using softmax regression, we can compute the posterior probability $P(w_O|w_I)$:\n",
    "\n",
    "$$ P(w_O|w_I) = y_i = \\frac{exp(W_I \\cdot W'^T_O)}{\\sum^V_{j=1} exp(W_I \\cdot W'^T_j)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the hidden-to-output layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function to minimize is: $E = -\\log P(w_O|w_I)$\n",
    "\n",
    "The error is computed with $t_j - P(w_O|w_I) = e_j$, where $t_j$ is 1 if $w_j$ is the actual output word, otherwise $t_j$ is 0.\n",
    "\n",
    "To obtain the gradient on the hidden-to-output weights, we compute $e_j \\cdot h_i$, where $h_i$ is a copy of the vector corresponding to the input word (only holds with a context of a single word). Finally, using stochastic gradient descent, with a learning rate $\\nu$ we obtain the weight update equation for the hidden to output layer weights:\n",
    "\n",
    "$$W'^{T (t)}_j = W'^{T (t-1)}_j - \\nu \\cdot e_j \\cdot h_j$$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the input-to-hidden layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backpropagate the prediction errors to the input-to-hidden weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-word context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = (WI[vocabulary[context[0]]] + WI[vocabulary[context[1]]]) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "class Word2Vec_nWordContext(object):\n",
    "    def __init__(self, sentences, learning_rate = 1.0, context_size = 3, nodes_HL = 3):\n",
    "        self.sentences = sentences\n",
    "        self.N = nodes_HL # number of nodes in Hidden Layer\n",
    "        self.V = None # Vocabulary size\n",
    "        self.WI = None # input weight matrix\n",
    "        self.WO = None # output weight matrix\n",
    "        self.vocabulary = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.context_size = context_size\n",
    "    \n",
    "    def Vocabulary(self):\n",
    "        dictionary = defaultdict()\n",
    "        # len of dictionary gives a unique integer to each new word\n",
    "        dictionary.default_factory = lambda: len(dictionary) \n",
    "        return dictionary\n",
    "\n",
    "    def docs2bow(self, docs, dictionary):\n",
    "        \"\"\"Transforms a list of strings into a list of lists where \n",
    "        each unique item is converted into a unique integer.\"\"\"\n",
    "        for doc in docs:\n",
    "            yield [dictionary[word] for word in doc.split()] # returns a generator\n",
    "    \n",
    "    def sentences2bow(self):\n",
    "        self.vocabulary = self.Vocabulary()\n",
    "        bow = list(self.docs2bow(self.sentences, self.vocabulary))\n",
    "        return bow\n",
    "    \n",
    "    def random_init(self):\n",
    "        self.V = len(self.vocabulary)\n",
    "        \n",
    "        # random initialization of weights between [-0.5 , 0.5] normalized by number of nodes mapping to.\n",
    "        self.WI =(np.random.random((self.V, self.N)) - 0.5) / self.N # input weights\n",
    "        self.WO =(np.random.random((self.N, self.V)) - 0.5) / self.V # output weights\n",
    "        \n",
    "    def average_context_vec(self, context):\n",
    "        c = len(context)\n",
    "        context_weights = map(lambda word: self.WI[self.vocabulary[word]], context)\n",
    "        return reduce(lambda a, b: a + b, context_weights ) / float(c)\n",
    "    \n",
    "    def softmax_regression(self, word, h):\n",
    "        # posterior probability P(word | context)\n",
    "        return (np.exp(h.dot(self.WO.T[self.vocabulary[word]])) / \n",
    "                sum(np.exp(h.dot(self.WO.T[self.vocabulary[w]])) for w in self.vocabulary))\n",
    "    \n",
    "    def backprop(self, context, target):\n",
    "\n",
    "        for word in self.vocabulary:\n",
    "            \n",
    "            h = self.average_context_vec(context)\n",
    "            P_word_context = self.softmax_regression(word, h)\n",
    "\n",
    "            if word == target:\n",
    "                t = 1\n",
    "                #print \"P(target|context)\", P_word_context\n",
    "            else:\n",
    "                t = 0\n",
    "\n",
    "            err = t - P_word_context # error\n",
    "\n",
    "            # weight update using stochastic gradient descent\n",
    "            self.WO.T[self.vocabulary[word]] -= self.learning_rate * err * h\n",
    "            # update brings word vector closer in the feature space if word = target, and push them apart otherwise.\n",
    "\n",
    "        for input_word in context:\n",
    "            # update only weights for context words\n",
    "            self.WI[self.vocabulary[input_word]] -= (1. / len(context)) * self.learning_rate * self.WO.sum(axis=1) \n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        bow = self.sentences2bow()\n",
    "        self.random_init()\n",
    "        \n",
    "        for sentence in self.sentences:\n",
    "            word_tuple =  tuple(sentence.split())\n",
    "            count = 1\n",
    "            context = []\n",
    "            for i, word in enumerate(word_tuple):\n",
    "                if word != '<s>':\n",
    "                    target = word\n",
    "                    if count > self.context_size:\n",
    "                        context = context[1:]\n",
    "                    context.append(word_tuple[i-1])\n",
    "                    self.backprop(context, target)\n",
    "                    if word == '</s>':\n",
    "                        for n in range(len(context) - 1, 0, -1):\n",
    "                            context = context[-n:]\n",
    "                            self.backprop(context, target)\n",
    "                    count += 1\n",
    "                    \n",
    "        return self.WI, OrderedDict(sorted(self.vocabulary.items(), key=lambda t: t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_tuple = tuple(['<s>', 'a', 'b', 'c', 'd', 'e', '</s>'])\n",
    "print word_tuple\n",
    "context_size = 4\n",
    "count = 1\n",
    "context = []\n",
    "\n",
    "for i, word in enumerate(word_tuple):\n",
    "    if word != '<s>':\n",
    "        target = word\n",
    "        if count > context_size:\n",
    "            context = context[1:]\n",
    "        context.append(word_tuple[i-1])         \n",
    "        print \"context: {}, target: {}\".format(context, target)\n",
    "        if word == '</s>':\n",
    "            for n in range(len(context)-1, 0, -1):\n",
    "                context = context[-n:]\n",
    "                print \"context: {}, target: {}\".format(context, target)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, learning_rate = 1.0, context_size = 4)\n",
    "W, vocab = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W =(np.random.random((14, 3)) - 0.5) / 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trace1 = Scatter3d(\n",
    "    x= W.T[0],\n",
    "    y= W.T[1],\n",
    "    z= W.T[2],\n",
    "    mode='markers+text',\n",
    "    text = vocab.keys(),\n",
    "    marker=Marker(\n",
    "        size=8,\n",
    "        line=Line(\n",
    "            color='rgba(217, 217, 217, 0.14)',\n",
    "            width=0.5\n",
    "        ),\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "data = Data([trace1])\n",
    "layout = Layout(\n",
    "    margin=Margin(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "class Doc2Vec_nWordContext(object):\n",
    "    def __init__(self, sentences, learning_rate = 1.0, context_size = 3, nodes_HL = 3):\n",
    "        self.sentences = sentences\n",
    "        self.N = nodes_HL # number of nodes in Hidden Layer\n",
    "        self.V = None # Vocabulary size\n",
    "        self.P = None # number of paragraph/sentence in text\n",
    "        self.WI = None # input weight matrix\n",
    "        self.WO = None # output weight matrix\n",
    "        self.D = None # paragraph/sentence weight matrix\n",
    "        self.vocabulary = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.context_size = context_size # number of words in context vector\n",
    "    \n",
    "    def Vocabulary(self):\n",
    "        \"\"\" Instantiates a default dictionary with its \n",
    "        length as default factory \"\"\"\n",
    "        dictionary = defaultdict()\n",
    "        # len of dictionary gives a unique integer to each new word\n",
    "        dictionary.default_factory = lambda: len(dictionary) \n",
    "        return dictionary\n",
    "\n",
    "    def docs2bow(self, docs, dictionary):\n",
    "        \"\"\"Transforms a list of strings into a list of lists where \n",
    "        each unique item is converted into a unique integer.\"\"\"\n",
    "        for doc in docs:\n",
    "            yield [dictionary[word] for word in doc.split()] # returns a generator\n",
    "    \n",
    "    def sentences2bow(self):\n",
    "        \"\"\" Creates the dictionary of the text's vocabulary \n",
    "        and returns the text with each words replaced by their unique integer\"\"\"\n",
    "        self.vocabulary = self.Vocabulary()\n",
    "        bow = list(self.docs2bow(self.sentences, self.vocabulary))\n",
    "        return bow\n",
    "    \n",
    "    def random_init(self):\n",
    "        \"\"\" initializes  weight matrices for neural network \"\"\"\n",
    "        self.V = len(self.vocabulary)\n",
    "        self.P = len(self.sentences)\n",
    "        \n",
    "        # random initialization of weights between [-0.5 , 0.5] normalized by number of nodes mapping to.\n",
    "        self.WI = (np.random.random((self.V, self.N)) - 0.5) / self.N # input weights\n",
    "        self.WO = (np.random.random((self.N, self.V)) - 0.5) / self.V # output weights\n",
    "        self.D = (np.random.random((self.P, self.N)) - 0.5) / self.N # paragraph/sentence weights\n",
    "        \n",
    "    def average_context_vec(self, context, num):\n",
    "        \"\"\" Takes the average of the context word vectors plus the paragraph/sentence vector\n",
    "        and returns a new vector for the context\"\"\"\n",
    "        c = len(context)\n",
    "        context_weights = map(lambda word: self.WI[self.vocabulary[word]], context)\n",
    "        return (reduce(lambda a, b: a + b, context_weights ) + self.D[num])/ float(c + 1)\n",
    "    \n",
    "    def softmax_regression(self, word, h):\n",
    "        \"\"\" returns posterior probability P(word | context) \"\"\"\n",
    "        return (np.exp(h.dot(self.WO.T[self.vocabulary[word]])) / \n",
    "                sum(np.exp(h.dot(self.WO.T[self.vocabulary[w]])) for w in self.vocabulary))\n",
    "    \n",
    "    def backprop(self, context, target, num):\n",
    "        \"\"\" Computes backpropagation of errors to weight matrices,\n",
    "        using stochastic gradient descent \"\"\"\n",
    "\n",
    "        for word in self.vocabulary:\n",
    "            \n",
    "            h = self.average_context_vec(context, num) # context word weight vector\n",
    "            P_word_context = self.softmax_regression(word, h)  # posterior probability P(word | context)\n",
    "\n",
    "            if word == target:\n",
    "                t = 1\n",
    "                #print \"P(target|context)\", P_word_context\n",
    "            else:\n",
    "                t = 0\n",
    "\n",
    "            err = t - P_word_context # error\n",
    "\n",
    "            # weight update using stochastic gradient descent\n",
    "            self.WO.T[self.vocabulary[word]] -= self.learning_rate * err * h\n",
    "            # update brings word vector closer in the feature space if word = target, and push them apart otherwise.\n",
    "\n",
    "        EH = self.WO.sum(axis = 1)\n",
    "        for input_word in context:\n",
    "            # update only weights for context words\n",
    "            self.WI[self.vocabulary[input_word]] -= (1. / len(context)) * self.learning_rate * EH\n",
    "            self.D[num] -= (1. / len(context)) * self.learning_rate * EH\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\" trains text and returns trained word matrix\n",
    "        and ordered dictionary of vocabulary\"\"\"\n",
    "\n",
    "        bow = self.sentences2bow()\n",
    "        self.random_init()\n",
    "        \n",
    "        # runs context window across sentence\n",
    "        # applies window expansion and reduction \n",
    "        # at the begin and end of sentence respectively\n",
    "        for num, sentence in enumerate(self.sentences):\n",
    "            word_tuple =  tuple(sentence.split())\n",
    "            count = 1\n",
    "            context = []\n",
    "            for i, word in enumerate(word_tuple):\n",
    "                if word != '<s>':\n",
    "                    target = word\n",
    "                    if count > self.context_size:\n",
    "                        context = context[1:]\n",
    "                    context.append(word_tuple[i-1])\n",
    "                    self.backprop(context, target, num)\n",
    "                    if word == '</s>':\n",
    "                        for n in range(len(context) - 1, 0, -1):\n",
    "                            context = context[-n:]\n",
    "                            self.backprop(context, target)\n",
    "                    count += 1\n",
    "                    \n",
    "        return self.WI, OrderedDict(sorted(self.vocabulary.items(), key=lambda t: t[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "backprop() takes exactly 4 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-af8045c1ef17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec_nWordContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6f2c7c8f96d7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m                     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: backprop() takes exactly 4 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec_nWordContext(sentences, learning_rate = 1.0, context_size = 2)\n",
    "W, vocab = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
