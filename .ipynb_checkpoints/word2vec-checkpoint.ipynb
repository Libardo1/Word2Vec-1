{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Models for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Word2Vec model is a simple word embedding neural network, developed by Mikolov et al. (2013).\n",
    "####  *Such continuous word embedding representations have have been proven to be able to carry semantic meanings and are useful in various NLP tasks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have attempted to implement three language models described in *Le & Mikolov (2014)*'s paper ** *Distributed Representations of Sentences and Documents* **.\n",
    "\n",
    "The implementations don't make use of any NLP libraries and consist of the simplest form of the algorithm without any optimization.\n",
    "The aim of this notebook is simply to gain:\n",
    "* understanding of the **language models' algorithm**\n",
    "* intuition on **word embedding representations**\n",
    "* understanding of inner workings of **neural networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, popular fixed-length features are **bag-of-words**.\n",
    "\n",
    "However bag-of-words features have two major weaknesses: \n",
    "- they lose the **ordering** of the words.\n",
    "- they also ignore **semantics** of the words. \n",
    "\n",
    "In the paper by **Le & Mikolov (2014)**, they propose a distributed representations of sentences and documents, which they call ** *Paragraph Vector* **. \n",
    "\n",
    "The model assumes the ** *Distributional Hypothesis* ** that words are characterized by other words in their vicinity. This idea is used to estimate the probability of two words occurring near each other.\n",
    "\n",
    "*Paragraph Vector* is an unsupervised algorithm that learns fixed-length feature representations from **variable-length pieces of texts**, such as sentences, paragraphs, and documents. The algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives the algorithm the potential to overcome the **weaknesses** of bag-of- words models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/3.png\"  width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical results, in the paper, show that **Paragraph Vectors outperform bag-of-words models**, as well as, other techniques for text representations. For example, **word weighting functions** (which requires task-specific tuning) and **parse trees**. The authors were able to achieve new **state-of-the-art** results on several **text classification** and **sentiment analysis** tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is made possible because words with **similar meaning** are mapped to a **similar position** in the **vector space**. The difference between word vectors also carry meaning. For example, the word vectors can be used to answer analogy questions using simple vector algebra: “King” - “man” + “woman” = “Queen”.\n",
    "Theses properties of word vectors are useful in many NLP task, such as **language modeling and understanding** and **machine translation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Word Context Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first implementation, I looked at the simplest form of the *Continuous Bag-Of_Word Model (CBOW)* introduced in *Mikolov et al. (2013a)*. This model has just **one word** as the context to predict the next target world in the sentence. This context vector is fed into a **neural network** with a **single hidden layer** that is fully connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image/1neural-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input vector is a one-hot encoded vector. Each word is mapped into its own unique vector of features, represented by a column in a matrix $W_I$. There is also another weight matrix, $W_O$ connecting the hidden layer to the output layer.\n",
    "\n",
    "Given a **context word** and these **weights matrices**, we can compute a **score** for each word in the vocabulary. This is essentially a **multiclass classification** where we have as many labels as our vocabulary size $V$. **Softmax regression**, a log-linear classification model, is used to compute the **posterior probability $P(W_O|W_I)$:**\n",
    "\n",
    "$$ P(W_O|W_I) = y_i = \\frac{exp(W_I \\cdot W'^T_O)}{\\sum^V_{j=1} exp(W_I \\cdot W'^T_j)} $$\n",
    "\n",
    "In the paper, hierarchical softmax is used for faster training, but for simpliticy I will just use a regular softmax regression as the multiclass classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continuous embeddings of the words represent **points in a vector space**. What the Word2Vec model is trying to do is assign each word a **meaningful point** in that space that represent its **semantic** in **relation** to the other words. The words are first **initialize a random location** in that space, then the model will learn better vector positions. Similar words will be **pushed together**, while different ones will be **pulled away.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the hidden-to-output layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model learns by maximzing the posterior probability. For numerical stability, we will instead minimize the log loss function: \n",
    "\n",
    "$$E = -\\log P(W_O|W_I)$$\n",
    "\n",
    "$$E = -\\log \\frac{exp(W_I \\cdot W'^T_O)}{\\sum^V_{j=1} exp(W_I \\cdot W'^T_j)} =-\\log \\frac{exp(u)}{\\sum^V_{j=1} exp(u_j)} $$\n",
    "\n",
    "$$E = -u + \\log \\sum^V_{j=1} exp(u_j)$$\n",
    "\n",
    "The update equation of the weights between hidden and output layers:\n",
    "\n",
    "$$\\dfrac{\\partial E}{\\partial u} = -t_j + \\frac{exp(u)}{\\sum^V_{j=1} exp(u_j)}$$\n",
    "$$ \\dfrac{\\partial E}{\\partial u} = P(W_O|W_I) - t_j = e_j$$\n",
    "\n",
    "Where $t_j$ is 1 if $w_j$ is the actual output word, otherwise $t_j$ is 0. This derivative is the prediction error of the output layer, $e_j$.\n",
    "\n",
    "The gradient on the hidden-to-output weights is:\n",
    "$$ \\dfrac{\\partial E}{\\partial W_O} = \\dfrac{\\partial E}{\\partial u_j} \\cdot \\dfrac{\\partial u_j}{\\partial W_O} = e_j \\cdot h_i$$\n",
    "\n",
    "Where $h_i$ is a copy of the vector corresponding to the input word. \n",
    "\n",
    "To update the  output weights for the hidden-to-output layer, stochastic gradient descent is used with a learning rate $\\alpha$: \n",
    "\n",
    "$${W_{O}}^{T (new)}_j = {W_{O}}^{T (old)}_j - \\alpha \\cdot e_j \\cdot h_j$$\n",
    "\n",
    "Using the dot product $W_I \\cdot W'^T_O$ we compute the distance between the input word *dwarf* and the output word *hates*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the input-to-hidden layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction errors need to be backpropagated to the input-to-hidden weights. \n",
    "The derivative of $E$ on the output of the hidden layer is:\n",
    "\n",
    "$$ \\dfrac{\\partial E}{\\partial h_i} = \\sum^V_{j=1} \\dfrac{\\partial E}{\\partial u_j} \\cdot \\dfrac{\\partial u_j}{\\partial h_i} = \\sum^V_{j=1} e_j \\cdot W_O = EH$$\n",
    "\n",
    "EH is represents the sum of the hidden-to-output vectors for each word in the vocabulary weighted by their prediction error\n",
    "\n",
    "The gradient on the input weights is:\n",
    "\n",
    "$$ \\dfrac{\\partial E}{\\partial W_I} = \\dfrac{\\partial E}{\\partial h_i} \\cdot \\dfrac{\\partial h_i}{\\partial W_I} = EH_i \\cdot x$$\n",
    "\n",
    "Where $x$ is one-hot encoded vector.\n",
    "\n",
    "To update the  intput weights for the input-to-hidden layer, we make use again of stochastic gradient descent, with a learning rate $\\alpha$: \n",
    "\n",
    "$${W_{I}}^{T (new)}_j = {W_{I}}^{T (old)}_j - \\alpha \\cdot EH $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have good idea how the model works, we'd like to see it in action. To keep this notebook concise and readable I have kept the code in separate files located in the same directory as this notebook.\n",
    "\n",
    "This first model can be found in file * \"Word2Vec_1WordContext.py\" *\n",
    "\n",
    "Let's test this model with a *tiny* toy dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = ['<s> the prince loves skateboarding in the park </s>', \n",
    "             '<s> the princess loves the prince but the princess hates skateboarding </s>',\n",
    "             '<s> skateboarding in the park is popular </s>',\n",
    "             '<s> the prince is popular but the prince hates attention </s>',\n",
    "             '<s> the princess loves attention but the princess hates the park </s>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should not expect to learn much with such a small sample. Furthermore, most neural networks perform very poorly on small datasets. However, the goal is to test if the model was implemented correctly and gain some intuition on how word vectors representation works. \n",
    "\n",
    "Tokens $<s>$  and $</s>$ have been added at the beginning and end of sentences respectively. This conveniently allows the context window to loop over every word equally.\n",
    "\n",
    "Let's import the file and instantiate the model with our tiny dataset with three nodes in the hidden layer and learning rate of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Word2Vec_1WordContext import Word2Vec_1WordContext\n",
    "\n",
    "model1 = Word2Vec_1WordContext(sentences, learning_rate = 1.2, nodes_HL = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is kept simple with only three hidden nodes because with such little data not much can be learned. In fact, we are actually better off constraining the model to low dimensionality in that case. In addition, three nodes will allow us to plot our word vectors on a 3D graph and visualize the neural network features.\n",
    "\n",
    "The training process has been simplified to a single iteration for every word in the text. The function returns the learned input weight matrix containing the word vectors and a dictionary of the text's vocabulary where the values are the corresponding row index of the weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary :\n",
      "('<s>', 0)\n",
      "('the', 1)\n",
      "('prince', 2)\n",
      "('loves', 3)\n",
      "('skateboarding', 4)\n",
      "('in', 5)\n",
      "('park', 6)\n",
      "('</s>', 7)\n",
      "('princess', 8)\n",
      "('but', 9)\n",
      "('hates', 10)\n",
      "('is', 11)\n",
      "('popular', 12)\n",
      "('attention', 13)\n",
      "\n",
      " Learned Word Vectors: \n",
      "[[  6.93514348e-01   2.22482953e-01  -6.52328523e-01]\n",
      " [  3.70990748e+00   1.68207222e+00  -4.67183651e+00]\n",
      " [  6.60541278e-01   2.36426330e-01  -4.83502806e-01]\n",
      " [  6.47420972e-01   7.79723073e-02  -6.93944016e-01]\n",
      " [  2.72237699e-01   8.73945718e-02  -3.24441008e-01]\n",
      " [  3.11279331e-01   3.26389162e-03  -3.64924873e-01]\n",
      " [  1.46959349e+00   5.52256398e-01  -1.83313983e+00]\n",
      " [ -7.79430576e-02   8.42628160e-02  -3.88375861e-02]\n",
      " [  1.80235484e+00   6.71521928e-01  -2.21456120e+00]\n",
      " [  6.92159815e-01   3.87727794e-01  -6.62886714e-01]\n",
      " [  1.53018930e+00   6.58563577e-01  -1.75762173e+00]\n",
      " [  1.63380476e-01   8.64797317e-02  -1.87342484e-01]\n",
      " [  3.00382280e-01   1.44069260e-01  -3.53623493e-01]\n",
      " [  5.78941765e-01   3.48969944e-01  -8.06500232e-01]]\n"
     ]
    }
   ],
   "source": [
    "W, vocab = model1.train()\n",
    "print \"Vocabulary :\"\n",
    "for kv in vocab.iteritems():\n",
    "    print kv\n",
    "print \"\\n Learned Word Vectors: \\n\", W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! You maybe trust me but you haven't gained any intuition on what has happened. That's the problem with neural nets. Fortunately, we have a very simple model with vectors constrained to just three features we can visualize them using the graphing function I incorporated in the class model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~DemoAccount/5047.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.graph_vector_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Word Context Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is an extention of the *Continuous Bag-Of_Word Model (CBOW)*. The context is now any number of words \n",
    "before the target word we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image/2neural-network-cbow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent this context of words, we simply take the average of the word vectors in this context. The hidden layer is now the product of this new vector and the input weight matrix: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h=\\frac{1}{C}W_{I}(x_1+x_2+…+x_C)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update for the output weights for the hidden-to-output layer is the same as before:\n",
    "\n",
    "$${W_{O}}^{T (new)}_j = {W_{O}}^{T (old)}_j - \\alpha \\cdot e_j \\cdot h_j$$\n",
    "\n",
    "The update for the input weights is almost identical as well. The only difference is that it now needs to be applied to every word in the context window.\n",
    "\n",
    "$${W_{I,c}}^{T (new)}_j = {W_{I,c}}^{T (old)}_j - \\frac{1}{C} \\cdot \\alpha \\cdot EH $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this second model can be found in file \"Word2Vec_nWordContext.py\"\n",
    "\n",
    "The learning algorithm is almost identical has the previous model. However, the challenge is now to find a way to allow the context window to shrink and expand.\n",
    "\n",
    "How do you deal with any size context window across any sentence size?\n",
    "To illustrate my point, I created a toy function that allows to visualize how does the context window moves across a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from context_window import context_window_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence example:  ('<s>', 'a', 'b', 'c', 'd', 'e', '</s>')\n",
      "context: ['<s>'], target: a\n",
      "context: ['<s>', 'a'], target: b\n",
      "context: ['a', 'b'], target: c\n",
      "context: ['b', 'c'], target: d\n",
      "context: ['c', 'd'], target: e\n",
      "context: ['d', 'e'], target: </s>\n",
      "context: ['e'], target: </s>\n"
     ]
    }
   ],
   "source": [
    "context_window_search(context_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the window is small and the sentence long this is straight forward, the window slides at every iteration. However when you have a large window and a small sentence, you will run in the problem of giving less importance to words at the beginning and end of the sentence.\n",
    "\n",
    "To get around that you need to dynamically change the size of the window as it slides across the sentence. The function below does this automatically. It expands gradually to the requested window size and then contracts as it reaches the end of the sentence. In this way, every word is passed as input excatly 5 times. You can check it for yourself below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence example:  ('<s>', 'a', 'b', 'c', 'd', 'e', '</s>')\n",
      "context: ['<s>'], target: a\n",
      "context: ['<s>', 'a'], target: b\n",
      "context: ['<s>', 'a', 'b'], target: c\n",
      "context: ['<s>', 'a', 'b', 'c'], target: d\n",
      "context: ['<s>', 'a', 'b', 'c', 'd'], target: e\n",
      "context: ['a', 'b', 'c', 'd', 'e'], target: </s>\n",
      "context: ['b', 'c', 'd', 'e'], target: </s>\n",
      "context: ['c', 'd', 'e'], target: </s>\n",
      "context: ['d', 'e'], target: </s>\n",
      "context: ['e'], target: </s>\n"
     ]
    }
   ],
   "source": [
    "context_window_search(context_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This little change allows for large imporvements in the model. The neural network can now learning and relate a word to both its adjacent and distant neighbors, allowing for better semantic understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Word2Vec_nWordContext import Word2Vec_nWordContext\n",
    "model2 = Word2Vec_nWordContext(sentences, learning_rate = 1.0, context_size = 3)\n",
    "W, vocab = model2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~DemoAccount/4999.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.graph_vector_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph Vector Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/3doc2vec.png\"  width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec_nWordContext(sentences, learning_rate = 1.0, context_size = 2)\n",
    "W, vocab = model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Results\n",
    "\n",
    "we concatenate the paragraph vector with several word vectors from a paragraph and predict the following word in the given context. Both word vectors and paragraph vectors are trained by the stochastic gradient descent and backpropaga- tion\n",
    "\n",
    "\n",
    "After training, the word vectors are mapped into a vector space such that semantically similar words have similar vector representations.\n",
    "In the study they extend the model to go beyond word level to achieve phrase-level or sentence-level representations. \n",
    "\n",
    "\n",
    "Paragraph Vector takes a general approach. It is capable of constructing representations of input sequences of any length."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
