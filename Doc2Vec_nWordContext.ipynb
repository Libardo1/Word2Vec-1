{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict, defaultdict\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "class Doc2Vec_nWordContext(object):\n",
    "    def __init__(self, sentences, learning_rate = 1.0, context_size = 3, nodes_HL = 3):\n",
    "        self.sentences = sentences\n",
    "        self.N = nodes_HL # number of nodes in Hidden Layer\n",
    "        self.V = None # Vocabulary size\n",
    "        self.P = None # number of paragraph/sentence in text\n",
    "        self.WI = None # input weight matrix\n",
    "        self.WO = None # output weight matrix\n",
    "        self.D = None # paragraph/sentence weight matrix\n",
    "        self.vocabulary = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.context_size = context_size # number of words in context vector\n",
    "    \n",
    "    def Vocabulary(self):\n",
    "        \"\"\" Instantiates a default dictionary with its \n",
    "        length as default factory \"\"\"\n",
    "        dictionary = defaultdict()\n",
    "        # len of dictionary gives a unique integer to each new word\n",
    "        dictionary.default_factory = lambda: len(dictionary) \n",
    "        return dictionary\n",
    "\n",
    "    def docs2bow(self, docs, dictionary):\n",
    "        \"\"\"Transforms a list of strings into a list of lists where \n",
    "        each unique item is converted into a unique integer.\"\"\"\n",
    "        for doc in docs:\n",
    "            yield [dictionary[word] for word in doc.split()] # returns a generator\n",
    "    \n",
    "    def sentences2bow(self):\n",
    "        \"\"\" Creates the dictionary of the text's vocabulary \n",
    "        and returns the text with each words replaced by their unique integer\"\"\"\n",
    "        self.vocabulary = self.Vocabulary()\n",
    "        bow = list(self.docs2bow(self.sentences, self.vocabulary))\n",
    "        return bow\n",
    "    \n",
    "    def random_init(self):\n",
    "        \"\"\" initializes  weight matrices for neural network \"\"\"\n",
    "        self.V = len(self.vocabulary)\n",
    "        self.P = len(self.sentences)\n",
    "        \n",
    "        # random initialization of weights between [-0.5 , 0.5] normalized by number of nodes mapping to.\n",
    "        self.WI = (np.random.random((self.V, self.N)) - 0.5) / self.N # input weights\n",
    "        self.WO = (np.random.random((self.N, self.V)) - 0.5) / self.V # output weights\n",
    "        self.D = (np.random.random((self.P, self.N)) - 0.5) / self.N # paragraph/sentence weights\n",
    "        \n",
    "    def average_context_vec(self, context, num):\n",
    "        \"\"\" Takes the average of the context word vectors plus the paragraph/sentence vector\n",
    "        and returns a new vector for the context\"\"\"\n",
    "        c = len(context)\n",
    "        context_weights = map(lambda word: self.WI[self.vocabulary[word]], context)\n",
    "        return (reduce(lambda a, b: a + b, context_weights ) + self.D[num])/ float(c + 1)\n",
    "    \n",
    "    def softmax_regression(self, word, h):\n",
    "        \"\"\" returns posterior probability P(word | context) \"\"\"\n",
    "        return (np.exp(h.dot(self.WO.T[self.vocabulary[word]])) / \n",
    "                sum(np.exp(h.dot(self.WO.T[self.vocabulary[w]])) for w in self.vocabulary))\n",
    "    \n",
    "    def backprop(self, context, target, num):\n",
    "        \"\"\" Computes backpropagation of errors to weight matrices,\n",
    "        using stochastic gradient descent \"\"\"\n",
    "\n",
    "        for word in self.vocabulary:\n",
    "            \n",
    "            h = self.average_context_vec(context, num) # context word weight vector\n",
    "            P_word_context = self.softmax_regression(word, h)  # posterior probability P(word | context)\n",
    "\n",
    "            if word == target:\n",
    "                t = 1\n",
    "                #print \"P(target|context)\", P_word_context\n",
    "            else:\n",
    "                t = 0\n",
    "\n",
    "            err = t - P_word_context # error\n",
    "\n",
    "            # weight update using stochastic gradient descent\n",
    "            self.WO.T[self.vocabulary[word]] -= self.learning_rate * err * h\n",
    "            # update brings word vector closer in the feature space if word = target, and push them apart otherwise.\n",
    "\n",
    "        EH = self.WO.sum(axis = 1)\n",
    "        for input_word in context:\n",
    "            # update only weights for context words\n",
    "            self.WI[self.vocabulary[input_word]] -= (1. / len(context)) * self.learning_rate * EH\n",
    "            \n",
    "            # update weights of paragraph vector\n",
    "            self.D[num] -= (1. / len(context)) * self.learning_rate * EH\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\" trains text and returns trained word matrix\n",
    "        and ordered dictionary of vocabulary\"\"\"\n",
    "\n",
    "        bow = self.sentences2bow()\n",
    "        self.random_init()\n",
    "        \n",
    "        # runs context window across sentence\n",
    "        # applies window expansion and reduction \n",
    "        # at the begin and end of sentence respectively\n",
    "        for num, sentence in enumerate(self.sentences):\n",
    "            word_tuple =  tuple(sentence.split())\n",
    "            count = 1\n",
    "            context = []\n",
    "            for i, word in enumerate(word_tuple):\n",
    "                if word != '<s>':\n",
    "                    target = word\n",
    "                    if count > self.context_size:\n",
    "                        context = context[1:]\n",
    "                    context.append(word_tuple[i-1])\n",
    "                    self.backprop(context, target, num)\n",
    "                    if word == '</s>':\n",
    "                        for n in range(len(context) - 1, 0, -1):\n",
    "                            context = context[-n:]\n",
    "                            self.backprop(context, target, num)\n",
    "                    count += 1\n",
    "                    \n",
    "        return self.WI, self.D, OrderedDict(sorted(self.vocabulary.items(), key=lambda t: t[1]))\n",
    "\n",
    "    def graph_vector_space(self):\n",
    "        \"\"\" 3D Scatter plot of first 3 word features with Plotly\"\"\"\n",
    "        vocab = OrderedDict(sorted(self.vocabulary.items(), key = lambda t: t[1]))\n",
    "        W_D = np.concatenate((self.WI, self.D), axis=0)\n",
    "        \n",
    "        text = vocab.keys()\n",
    "        for i in range(len(self.sentences)):\n",
    "            text.append('S' + str(i+1))\n",
    "        \n",
    "        trace1 = Scatter3d(\n",
    "            x = W_D.T[0],\n",
    "            y = W_D.T[1],\n",
    "            z = W_D.T[2],\n",
    "            mode ='markers+text',\n",
    "            text = text,\n",
    "            marker = Marker(\n",
    "                size = 8,\n",
    "                line = Line(\n",
    "                    color = 'rgba(217, 217, 217, 0.14)',\n",
    "                    width = 0.5\n",
    "                ),\n",
    "                opacity = 0.8\n",
    "            )\n",
    "        )\n",
    "        data = Data([trace1])\n",
    "        layout = Layout(\n",
    "            margin = Margin(\n",
    "                l = 0,\n",
    "                r = 0,\n",
    "                b = 0,\n",
    "                t = 0\n",
    "            )\n",
    "        )\n",
    "        fig = Figure(data = data, layout = layout)\n",
    "        return py.iplot(fig)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sentences = ['<s> the prince loves skateboarding in the park </s>', \n",
    "                 '<s> the princess loves the prince but the princess hates skateboarding </s>',\n",
    "                 '<s> skateboarding in the park is popular </s>',\n",
    "                 '<s> the prince is popular but the prince hates attention </s>',\n",
    "                 '<s> the princess loves attention but the princess hates the park </s>']\n",
    "\n",
    "    model = Doc2Vec_nWordContext(sentences, learning_rate = 1.0, context_size = 3)\n",
    "    WI, D, vocab = model.train()\n",
    "    print WI, D, vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
