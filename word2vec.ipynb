{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Models for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Word2Vec model is a simple word embedding neural network, developed by Mikolov et al. (2013).\n",
    "####  *Such continuous word embedding representations have have been proven to be able to carry semantic meanings and are useful in various NLP tasks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have attempted to implement three language models described in *Le & Mikolov (2014)*'s paper ** *Distributed Representations of Sentences and Documents* **.\n",
    "\n",
    "The implementations don't make use of any NLP libraries and consist of the simplest form of the algorithm with little optimization.\n",
    "The aim of this notebook is simply to gain:\n",
    "* understanding of the **language models' algorithm**\n",
    "* intuition on **word embedding representations**\n",
    "* understanding of inner workings of **neural networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, popular fixed-length features used in language models are **bag-of-words**.\n",
    "\n",
    "However bag-of-words features have two major weaknesses: \n",
    "- they lose the **ordering** of the words.\n",
    "- they also ignore **semantics** of the words. \n",
    "\n",
    "In the paper by **Le & Mikolov (2014)**, they propose a distributed representation of sentences and documents, which they call ** *Paragraph Vector* **. \n",
    "\n",
    "The model assumes the ** *Distributional Hypothesis* ** that words are characterized by other words in their vicinity. This idea is used to estimate the probability of two words occurring near each other.\n",
    "\n",
    "*Paragraph Vector* is an **unsupervised algorithm** that learns fixed-length feature representations from **variable-length pieces of texts**, such as sentences and paragraphs. The algorithm represents each word and sentence by a dense vector which is trained to predict words in the document. Its construction gives the algorithm the potential to overcome the **weaknesses** of bag-of- words models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/3.png\"  width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical results, in the paper, show that **Paragraph Vectors outperform bag-of-words models**, as well as, other techniques for text representations. For example, **word weighting functions** (which requires task-specific tuning) and **parse trees**. The authors were able to achieve new **state-of-the-art** results on several **text classification** and **sentiment analysis** tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is made possible because words with **similar meaning** are mapped to a **similar position** in the **vector space**. The difference between word vectors also carry meaning. For example, the word vectors can be used to answer analogy questions using simple vector algebra: “King” - “man” + “woman” = “Queen”.\n",
    "Theses properties of word vectors are useful in many NLP task, such as **language modeling and understanding** and **machine translation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Word Context Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this  first language model, we will look at the simplest form of the *Continuous Bag-Of-Word Model (CBOW)* introduced in *Mikolov et al. (2013a)*. This model has just **one word** as the context to predict the next target word in the sentence. This context vector is fed into a **neural network** with a **single hidden layer** that is fully connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image/1neural-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input vector is a **one-hot encoded vector**. Each word is mapped into its own **unique vector of features**, represented by a column in the matrix $W_I$. There is also another weight matrix, $W_O$ connecting the hidden layer to the output layer.\n",
    "\n",
    "Given a **context word** and these **weight matrices**, we can compute a **score** for each word in the vocabulary. This is essentially a **multiclass classification** where we have as many labels as our vocabulary size $V$. **Softmax regression**, a log-linear classification model, is used to compute the **posterior probability $P(W_O|W_I)$:**\n",
    "\n",
    "$$ P(W_O|W_I) = y_i = \\frac{exp(W_I \\cdot W'^T_O)}{\\sum^V_{j=1} exp(W_I \\cdot W'^T_j)} $$\n",
    "\n",
    "In the paper, **hierarchical softmax** is used for faster training, but for simpliticy I will just use a regular softmax regression as the multiclass classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continuous embeddings of the words represent **points in a vector space**. What the Word2Vec model is trying to do is assign each word a **meaningful point** in that space that represent its **semantic** in **relation** to the other words. The words are first **initialized to a random location** in that space, the model will then try  to learn better vector positions. Similar words will be **pushed together**, while different ones will be **pulled away.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the hidden-to-output layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model learns by maximzing the posterior probability. For **numerical stability**, we will instead **minimize the log loss function**: \n",
    "\n",
    "$$E = -\\log P(W_O|W_I)$$\n",
    "\n",
    "$$E = -\\log \\frac{exp(W_I \\cdot W'^T_O)}{\\sum^V_{j=1} exp(W_I \\cdot W'^T_j)} =-\\log \\frac{exp(u)}{\\sum^V_{j=1} exp(u_j)} $$\n",
    "\n",
    "$$E = -u + \\log \\sum^V_{j=1} exp(u_j)$$\n",
    "\n",
    "The update equation of the weights between **hidden and output layers**:\n",
    "\n",
    "$$\\dfrac{\\partial E}{\\partial u} = -t_j + \\frac{exp(u)}{\\sum^V_{j=1} exp(u_j)}$$\n",
    "$$ \\dfrac{\\partial E}{\\partial u} = P(W_O|W_I) - t_j = e_j$$\n",
    "\n",
    "Where $t_j = 1$ if the **output word is the actual target**, otherwise $t_j = 0$. This derivative is also the **prediction error of the output layer**, $e_j$.\n",
    "\n",
    "The **gradient** on the hidden-to-output weights is:\n",
    "$$ \\dfrac{\\partial E}{\\partial W_O} = \\dfrac{\\partial E}{\\partial u_j} \\cdot \\dfrac{\\partial u_j}{\\partial W_O} = e_j \\cdot h_i$$\n",
    "\n",
    "Where $h_i$ is a copy of the vector corresponding to the **input word**. \n",
    "\n",
    "To **update the  output weights**, $W_O$, for the hidden-to-output layer, **stochastic gradient descent** is used with a learning rate $\\alpha$: \n",
    "\n",
    "$${W_{O}}^{T (new)}_j = {W_{O}}^{T (old)}_j - \\alpha \\cdot e_j \\cdot h_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the input-to-hidden layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **prediction errors** need to be **backpropagated** to the input-to-hidden weights. \n",
    "\n",
    "The derivative of $E$ on the output of the hidden layer is:\n",
    "\n",
    "$$ \\dfrac{\\partial E}{\\partial h_i} = \\sum^V_{j=1} \\dfrac{\\partial E}{\\partial u_j} \\cdot \\dfrac{\\partial u_j}{\\partial h_i} = \\sum^V_{j=1} e_j \\cdot W_O = EH$$\n",
    "\n",
    "EH is the **sum of the hidden-to-output vectors** for each word in the vocabulary **weighted** by their **prediction error**.\n",
    "\n",
    "The **gradient** on the input weights is:\n",
    "\n",
    "$$ \\dfrac{\\partial E}{\\partial W_I} = \\dfrac{\\partial E}{\\partial h_i} \\cdot \\dfrac{\\partial h_i}{\\partial W_I} = EH_i \\cdot x$$\n",
    "\n",
    "Where $x$ is one-hot encoded vector.\n",
    "\n",
    "To **update the  intput weights** for the input-to-hidden layer, we make use again of **stochastic gradient descent**, with a learning rate $\\alpha$: \n",
    "\n",
    "$${W_{I}}^{T (new)}_j = {W_{I}}^{T (old)}_j - \\alpha \\cdot EH $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have good idea how the model works, we'd like to see it in action. To keep this notebook **concise and readable** I have kept the code in separate files located in the same directory of this notebook.\n",
    "\n",
    "This **first model** can be found in file * \"Word2Vec_1WordContext.py\" *\n",
    "\n",
    "Let's test this model with a *tiny* toy dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = ['<s> the prince loves skateboarding in the park </s>', \n",
    "             '<s> the princess loves the prince but the princess hates skateboarding </s>',\n",
    "             '<s> skateboarding in the park is popular </s>',\n",
    "             '<s> the prince is popular but the prince hates attention </s>',\n",
    "             '<s> the princess loves attention but the princess hates the park </s>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should not expect to learn much with such a small sample. Furthermore, most **neural networks** perform very poorly on small datasets. However, the goal is to test if the model was implemented correctly and gain some **intuition** on how word vectors representation works. \n",
    "\n",
    "Tokens $<s>$  and $</s>$ have been added at the beginning and end of sentences respectively. This conveniently allows the context window to loop over every word **equally** (more on that later).\n",
    "\n",
    "Let's import the file and **instantiate the model** with our tiny dataset with **three nodes** in the hidden layer and learning rate of 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Word2Vec_1WordContext import Word2Vec_1WordContext\n",
    "\n",
    "model1 = Word2Vec_1WordContext(sentences, learning_rate = 1.2, nodes_HL = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is kept simple with only **three hidden nodes** because with such little data not much can be learned. In fact, we are actually better off constraining the model to **low dimensionality** in that case. In addition, three nodes will allow us to plot our word vectors on a **3D graph** and **visualize the neural network features**.\n",
    "\n",
    "The training process has been simplified to a **single iteration for every word in the text**. The function returns the learned **input weight matrix** containing the **word vectors**, and a dictionary of the **text's vocabulary** where the values are the corresponding **row indices** of the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary :\n",
      "('<s>', 0)\n",
      "('the', 1)\n",
      "('prince', 2)\n",
      "('loves', 3)\n",
      "('skateboarding', 4)\n",
      "('in', 5)\n",
      "('park', 6)\n",
      "('</s>', 7)\n",
      "('princess', 8)\n",
      "('but', 9)\n",
      "('hates', 10)\n",
      "('is', 11)\n",
      "('popular', 12)\n",
      "('attention', 13)\n",
      "\n",
      " Learned Word Vectors: \n",
      "[[-0.24525522 -0.73506306 -1.04499952]\n",
      " [-1.59235337 -5.73741904 -6.90140056]\n",
      " [-0.21372206 -0.76843203 -0.98386624]\n",
      " [-0.28341044 -1.35226618 -1.62370164]\n",
      " [-0.26873672 -0.18208689 -0.56147579]\n",
      " [-0.03481274 -0.23087481 -0.32107108]\n",
      " [-0.40130398 -1.72378896 -2.37189492]\n",
      " [-0.16403185  0.1048265   0.07316326]\n",
      " [-0.81311322 -2.98261138 -3.681048  ]\n",
      " [-0.53568263 -1.56086392 -1.86244856]\n",
      " [-0.71411839 -2.24031558 -2.51425813]\n",
      " [-0.28805991 -0.21858411 -0.48678974]\n",
      " [-0.21415675 -0.40542378 -0.40006184]\n",
      " [-0.33141992 -1.59899401 -1.8707565 ]]\n"
     ]
    }
   ],
   "source": [
    "W, vocab = model1.train()\n",
    "\n",
    "print \"Vocabulary :\"\n",
    "for kv in vocab.iteritems():\n",
    "    print kv\n",
    "print \"\\n Learned Word Vectors: \\n\", W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It **worked!** You maybe trust me, but you haven't gained any **intuition** on what has happened. That's the **problem** with **neural nets** they are **hard to interpret**. Fortunately, we have a very **simple model** with vectors constrained to just **three features**. Therefore, we can **visualize** them using the graphing function I incorporated in the class model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~DemoAccount/5093.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.graph_vector_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was train with very **little data** over a **small number of iterations**, therefore based on how the vectors where **initionalized** the **3D scatter plot** will **differ** a little every time you run it.\n",
    "\n",
    "Nonetheless, there is a **lot to notice** in the **representation** created by the **neural network**. Firstly, we can observe that the words are definitely **not randomly positioned**. The **word vectors** have been **moved** by the algorithm into so kind of a **structure**. \n",
    "\n",
    "For example, the tokens *\"the\"* and $</s>$ are found at both **opposite extremes** of the plot. The fact that *\"the\"* was often found at the beginning of a sentence and $</s>$ at the end, may explain this **relationship**.\n",
    "\n",
    "Furthermore, words like *\"skateboarding\"*, *\"popular\"* and *\"prince\"* are **clustered together**, whereas *\"princess\"*, *\"hates\"* and *\"but\"* form **another cluster**. From different angles, we can observe **additional structures** like *\"is popular\"* and *\"loves attention\"*.\n",
    "\n",
    "These are already pretty **impressive results**, considering how **little data** the model had. It was able to some extent capture some of the **semantics of the text**.\n",
    "\n",
    "Let's see if we can do even **better!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Word Context Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is an extention of the *Continuous Bag-Of_Word Model (CBOW)*. The context is now any **number of words**\n",
    "before the **target word** we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image/2neural-network-cbow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent this **context of words**, we simply take the **average of the word vectors** in the context. The hidden layer is now the product of this **new vector** and the **input weight matrix**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h=\\frac{1}{C}W_{I}(x_1+x_2+…+x_C)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update for the output weights for the hidden-to-output layer is the same as before:\n",
    "\n",
    "$${W_{O}}^{T (new)}_j = {W_{O}}^{T (old)}_j - \\alpha \\cdot e_j \\cdot h_j$$\n",
    "\n",
    "The update for the input weights is almost identical as well. The only difference is that it now needs to be applied to **every word in the context window**.\n",
    "\n",
    "$${W_{I,c}}^{T (new)}_j = {W_{I,c}}^{T (old)}_j - \\frac{1}{C} \\cdot \\alpha \\cdot EH $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this **second model** can be found in file \"Word2Vec_nWordContext.py\"\n",
    "\n",
    "The learning algorithm is almost identical to the previous model. However, the challenge is now to find a way to allow the **context window to shrink and expand** as it loops over sentences.\n",
    "\n",
    "How do you deal with **any size context window** across **any sentence size**?\n",
    "To illustrate my point, I've created a toy function that helps to visualize how does the **context window moves across a sentence**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from context_window import context_window_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence example:  ('<s>', 'a', 'b', 'c', 'd', 'e', '</s>')\n",
      "context: ['<s>'], target: a\n",
      "context: ['<s>', 'a'], target: b\n",
      "context: ['a', 'b'], target: c\n",
      "context: ['b', 'c'], target: d\n",
      "context: ['c', 'd'], target: e\n",
      "context: ['d', 'e'], target: </s>\n",
      "context: ['e'], target: </s>\n"
     ]
    }
   ],
   "source": [
    "context_window_search(context_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, when the **window is small** and the **sentence long** this is straight forward. The **window slides** at every iteration. However when you have a **large window** and a **small sentence**, you will run in the problem of giving **less importance** to words at the **beginning and end** of the sentence.\n",
    "\n",
    "To get around that you need to **dynamically** change the **size of the window** as it slides across the sentence. The function below does this automatically. \n",
    "\n",
    "It **expands** gradually to the requested window size and then **contracts** as it reaches the end of the sentence. In this way, every word is passed as input excatly **5 times** for a context size of 5 for example. You can check it for yourself below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence example:  ('<s>', 'a', 'b', 'c', 'd', 'e', '</s>')\n",
      "context: ['<s>'], target: a\n",
      "context: ['<s>', 'a'], target: b\n",
      "context: ['<s>', 'a', 'b'], target: c\n",
      "context: ['<s>', 'a', 'b', 'c'], target: d\n",
      "context: ['<s>', 'a', 'b', 'c', 'd'], target: e\n",
      "context: ['a', 'b', 'c', 'd', 'e'], target: </s>\n",
      "context: ['b', 'c', 'd', 'e'], target: </s>\n",
      "context: ['c', 'd', 'e'], target: </s>\n",
      "context: ['d', 'e'], target: </s>\n",
      "context: ['e'], target: </s>\n"
     ]
    }
   ],
   "source": [
    "context_window_search(context_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This little change allows for **large improvements** in the model. The neural network can now **learning and relate** a word to both its **adjacent** and **distant neighbors**, allowing for **better semantic understanding**.\n",
    "\n",
    "Let's train this new model on our tiny dataset with this time a **context window of three words**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Word2Vec_nWordContext import Word2Vec_nWordContext\n",
    "model2 = Word2Vec_nWordContext(sentences, learning_rate = 1.1, context_size = 3, nodes_HL = 3)\n",
    "W, vocab = model2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~DemoAccount/5085.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.graph_vector_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the model **pushed the word vectors** into a **structured vector space**. Some of the same previous strucutres are found here. We can also notice, if viewed the space from **different angles**, that more **subtle relationships** are starting to appear. \n",
    "\n",
    "For example, *\"princess\" - \"prince\"* **approximate** to *\"hates\" - \"loves\"*. In some sense, the model was able to capture **relations** between words that can be mapped into **mathematical operations**.\n",
    "\n",
    "To be fair, this is not completely obvious from the plot, and some of it may very well just be due to randomness in this case. However as you increase the **data size**, the **search space dimentionality** and **training time**, this model will be able to extract **deeper meaning and semantinc** from a text **efficently**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph Vector Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Paragraph Vector model is another variation of the previous models. In addition to **mapping every word** into **unique vectors**, the model will also **map the paragraphs or sentences** in the text into a **unique vector**. Each represented by a column in matrix $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/3doc2vec.png\"  width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only real difference in the algorithm is with the **computation of the hidden layer**, $h$. Now both the **sentence vector and word vectors** are **averaged** to predict the next word in a context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h=\\frac{1}{C}W_{I}(D_d+x_1+x_2+…+x_C)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the **sentence token** as just another **vocabulary word**. However, this token vector tries to represent the **semantics of the entire sentence**. In other words, it acts as a **memory that remembers what is missing from the current context**. For this reason, this model is also referred as the ** *Distributed Memory Model of Paragraph Vectors (PV-DM)* **.\n",
    "\n",
    "As the context window travels across a sentence, the **sentence vector is unique**, whereas the **context vector changes** constantly. After being trained, the paragraph vectors can be used as **features** for the paragraph instead of **additional bag-of-words**. These features can then be used directly in other **machine learning models**, such as logistic regression or K-means.\n",
    "\n",
    "This allows the model to perform **better than a bag-of-n-grams model** because it is able to **generalize the text features** without **suffering from very high-dimensional representations** like in a bag of n-grams model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of this **third language model** was fairly straight forward, but **concenptually** the **extra paragraph token** added has the potential to greatly boost its **learning capabilities**.\n",
    "\n",
    "Let's train on our tiny dataset one last time and see the difference this model makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Doc2Vec_nWordContext import Doc2Vec_nWordContext\n",
    "model3 = Doc2Vec_nWordContext(sentences, learning_rate = 0.8, context_size = 3, nodes_HL = 3)\n",
    "W, D, vocab = model3.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function now also returns the $D$ weight matrix, which is the **learned representation of each five sentences in the text**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08648856, -0.88055301, -0.57307792],\n",
       "       [ 0.32213782, -1.27448826, -0.76827542],\n",
       "       [ 0.07848751, -0.85901126, -0.71087223],\n",
       "       [ 0.2065279 , -1.14088364, -0.88300124],\n",
       "       [ 0.64365723, -1.99965263, -1.09778346]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~DemoAccount/5101.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.graph_vector_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The plot of the learned word vectors, now also include the **sentence representations** (S1, S2, etc.). \n",
    "\n",
    "The first thing we can notice is that the sentence vectors have been **physically separated** from the word vectors. The model was able to **differentiate** between **individual words** and **entire sections of the text**. \n",
    "\n",
    "Furthermore, sentence vectors are grouped into a **strucutre of their own**. **S1 and S3** are together while **S2 and S4** are in another region. The first group of sentences are about **skateboarding in the park**, while the other cluster have sentences with very **similar grammatical structure**. They **compare** what the prince and princess love and hate.\n",
    "\n",
    "As discussed earlier the Paragraph Vector model takes a **general approach** and is capable of having some sort of **memory**. Therefore it can divide a text into **topics** in an **un-surpervised** way.\n",
    "\n",
    "This is **powerful** idea which allows to learn a text at **different levels or scales** (words, sentences, paragraphs). In fact, this is similar to **convolution nets** that use **max pooling** to **zoom out** of an image and learning different **feature abstractions** embedded in the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scale up to real datasets\n",
    "* Learn about NLP libraries like gensim\n",
    "* Move to more sophisticated models (hierarchical softmax, negative sampling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
