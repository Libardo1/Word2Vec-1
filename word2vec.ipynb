{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Word2Vec\n",
    "#### The model Word2Vec is a simple word embedding neural network with a single hidden layer, based on the study of Le & Mikolov (2014)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings from plain text.\n",
    "\n",
    "The model assumes the *Distributional Hypothesis* that words are characterized by words they hang out with. this idea is used to estimate the probability of two words occurring near each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is a very simple neural network with a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = ['the king loves the queen', 'the queen loves the king',\n",
    "             'the dwarf hates the king', 'the queen hates the dwarf',\n",
    "             'the dwarf poisons the king', 'the dwarf poisons the queen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def Vocabulary():\n",
    "    dictionary = defaultdict()\n",
    "    dictionary.default_factory = lambda: len(dictionary) # len of dictionary gives a unique integer to each new word\n",
    "    return dictionary\n",
    "\n",
    "def docs2bow(docs, dictionary):\n",
    "    \"\"\"Transforms a list of strings into a list of lists where \n",
    "    each unique item is converted into a unique integer.\"\"\"\n",
    "    for doc in docs:\n",
    "        yield [dictionary[word] for word in doc.split()] # returns a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 0, 3],\n",
       " [0, 3, 2, 0, 1],\n",
       " [0, 4, 5, 0, 1],\n",
       " [0, 3, 5, 0, 4],\n",
       " [0, 4, 6, 0, 1],\n",
       " [0, 4, 6, 0, 3]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = Vocabulary()\n",
    "sentences_bow = list(docs2bow(sentences, vocabulary))\n",
    "sentences_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>>,\n",
       "            {'dwarf': 4,\n",
       "             'hates': 5,\n",
       "             'king': 1,\n",
       "             'loves': 2,\n",
       "             'poisons': 6,\n",
       "             'queen': 3,\n",
       "             'the': 0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = len(vocabulary)\n",
    "N = 3 # number of nodes in Hidden Layer\n",
    "\n",
    "# random initialization of weights between [-0.5 , 0.5] normalized by number of nodes mapping to.\n",
    "WI =(np.random.random((V, N)) - 0.5) / N # input weights\n",
    "WO =(np.random.random((N, V)) - 0.5) / V # output weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06259958 -0.04045636 -0.00231579]\n",
      " [-0.14813526  0.03140125  0.07526604]\n",
      " [-0.09669949  0.02225064  0.07541938]\n",
      " [-0.0804188  -0.06866706  0.12916966]\n",
      " [-0.06829591 -0.07356343 -0.16079142]\n",
      " [ 0.07614897  0.05643023 -0.10641049]\n",
      " [ 0.10718903  0.1422684   0.08797442]]\n"
     ]
    }
   ],
   "source": [
    "print WI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04535321  0.05662719  0.00660368  0.03789646  0.0420766  -0.03693758\n",
      "  -0.02067386]\n",
      " [-0.06374536  0.00364524  0.01388948  0.06994418 -0.02437946  0.02687771\n",
      "  -0.03953408]\n",
      " [ 0.06046383  0.06506942  0.03423023  0.06397297 -0.0050463  -0.00836647\n",
      "  -0.03639222]]\n"
     ]
    }
   ],
   "source": [
    "print WO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06829591, -0.07356343, -0.16079142])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input weights associated with dwarf\n",
    "WI[vocabulary['dwarf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03693758,  0.02687771, -0.00836647])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output weights associated with hates\n",
    "WO.T[vocabulary['hates']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dot product $W_I \\cdot W'^T_O$ we compute the distance between the input word *dwarf* and the output word *hates*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0018907259741998627"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WI[vocabulary['dwarf']].dot(WO.T[vocabulary['hates']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using softmax regression, we can compute the posterior probability $P(w_O|w_I)$:\n",
    "\n",
    "$$ P(w_O|w_I) = y_i = \\frac{exp(W_I \\cdot W'^T_O)}{\\sum^V_{j=1} exp(W_I \\cdot W'^T_j)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1437309461247088"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerator = np.exp(WI[vocabulary['dwarf']].dot(WO.T[vocabulary['hates']]))\n",
    "denominator = sum(np.exp(WI[vocabulary['dwarf']].dot(WO.T[vocabulary[word]])) for word in vocabulary)\n",
    "\n",
    "P_hates_dwarf = numerator / denominator\n",
    "P_hates_dwarf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the hidden-to-output layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function to minimize is: $E = -\\log P(w_O|w_I)$\n",
    "\n",
    "The error is computed with $t_j - P(w_O|w_I) = e_j$, where $t_j$ is 1 if $w_j$ is the actual output word, otherwise $t_j$ is 0.\n",
    "\n",
    "To obtain the gradient on the hidden-to-output weights, we compute $e_j \\cdot h_i$, where $h_i$ is a copy of the vector corresponding to the input word (only holds with a context of a single word). Finally, using stochastic gradient descent, with a learning rate $\\nu$ we obtain the weight update equation for the hidden to output layer weights:\n",
    "\n",
    "$$W'^{T (t)}_j = W'^{T (t-1)}_j - \\nu \\cdot e_j \\cdot h_j$$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09218151  0.33392974 -0.03962089 -0.0082109  -0.00390225 -0.08302784\n",
      "  -0.06674708]\n",
      " [-0.10373055  0.24042508 -0.02558022  0.03057457 -0.06363934 -0.0124773\n",
      "  -0.07887454]\n",
      " [ 0.13568001 -0.38033732  0.10847671  0.13803118  0.06880549  0.06566426\n",
      "   0.03761114]]\n"
     ]
    }
   ],
   "source": [
    "target_word = 'king' \n",
    "input_word = 'queen' # context word\n",
    "learning_rate = 1.0\n",
    "\n",
    "denominator = sum(np.exp(WI[vocabulary[input_word]].dot(WO.T[vocabulary[word]])) for word in vocabulary)\n",
    "\n",
    "for word in vocabulary:\n",
    "    \n",
    "    numerator = np.exp(WI[vocabulary[input_word]].dot(WO.T[vocabulary[word]]))\n",
    "    P_word_queen = numerator / denominator # posterior probability P(word | queen)\n",
    "    \n",
    "    if word == target_word:\n",
    "        t = 1\n",
    "    else:\n",
    "        t = 0\n",
    "    \n",
    "    err = t - P_word_queen # error\n",
    "    \n",
    "    # weight update using stochastic gradient descent\n",
    "    WO.T[vocabulary[word]] -= learning_rate * err * WI[vocabulary[input_word]]\n",
    "    # update brings word vector closer in the feature space if word = target, and push them apart otherwise.\n",
    "    \n",
    "print WO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the input-to-hidden layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backpropagate the prediction errors to the input-to-hidden weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
