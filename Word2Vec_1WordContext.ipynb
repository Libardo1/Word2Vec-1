{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.21687754  0.64730296  0.0568644 ]\n",
      " [-0.74366069  1.48105954 -0.13055145]\n",
      " [-0.28996636  0.53366388  0.03009047]\n",
      " [-0.20414704  0.36278694 -0.06282324]\n",
      " [-0.30416123  0.26214707 -0.09185097]\n",
      " [-0.12670539  0.34018649 -0.02827352]\n",
      " [-0.31564714  0.63847254 -0.05446226]\n",
      " [ 0.13710635 -0.08301046  0.04903543]\n",
      " [-0.25135685  0.47754648 -0.00496529]\n",
      " [-0.07332236  0.52204515 -0.03103327]\n",
      " [-0.1377697   0.34646141 -0.0235496 ]\n",
      " [ 0.00559691  0.33425194  0.11107177]\n",
      " [-0.19846633  0.30149366 -0.00765594]\n",
      " [-0.17620046  0.4368966  -0.12427068]] OrderedDict([('<s>', 0), ('the', 1), ('prince', 2), ('loves', 3), ('skateboarding', 4), ('in', 5), ('park', 6), ('</s>', 7), ('princess', 8), ('but', 9), ('hates', 10), ('is', 11), ('popular', 12), ('attention', 13)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict, defaultdict\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "class Word2Vec_1WordContext(object):\n",
    "    def __init__(self, sentences, learning_rate = 1.0, nodes_HL = 3):\n",
    "        self.sentences = sentences\n",
    "        self.N = nodes_HL # number of nodes in Hidden Layer\n",
    "        self.V = None # Vocabulary size\n",
    "        self.WI = None # input weight matrix\n",
    "        self.WO = None # output weight matrix\n",
    "        self.vocabulary = None\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def Vocabulary(self):\n",
    "        \"\"\" Instantiates a default dictionary with its \n",
    "        length as default factory \"\"\"\n",
    "        dictionary = defaultdict()\n",
    "        # len of dictionary gives a unique integer to each new word\n",
    "        dictionary.default_factory = lambda: len(dictionary) \n",
    "        return dictionary\n",
    "\n",
    "    def docs2bow(self, docs, dictionary):\n",
    "        \"\"\"Transforms a list of strings into a list of lists where \n",
    "        each unique item is converted into a unique integer.\"\"\"\n",
    "        for doc in docs:\n",
    "            yield [dictionary[word] for word in doc.split()] # returns a generator\n",
    "    \n",
    "    def sentences2bow(self):\n",
    "        \"\"\" Creates the dictionary of the text's vocabulary \n",
    "        and returns the text with each words replaced by their unique integer\"\"\"\n",
    "        self.vocabulary = self.Vocabulary()\n",
    "        bow = list(self.docs2bow(self.sentences, self.vocabulary))\n",
    "        return bow\n",
    "    \n",
    "    def random_init(self):\n",
    "        \"\"\" initializes  weight matrices for neural network \"\"\"\n",
    "        self.V = len(self.vocabulary)\n",
    "        \n",
    "        # random initialization of weights between [-0.5 , 0.5] normalized by number of nodes mapping to.\n",
    "        self.WI =(np.random.random((self.V, self.N)) - 0.5) / self.N # input weights\n",
    "        self.WO =(np.random.random((self.N, self.V)) - 0.5) / self.V # output weights\n",
    "    \n",
    "    def softmax_regression(self, word, h):\n",
    "        \"\"\" returns posterior probability P(word | context) \"\"\"\n",
    "        return (np.exp(h.dot(self.WO.T[self.vocabulary[word]])) / \n",
    "                sum(np.exp(h.dot(self.WO.T[self.vocabulary[w]])) for w in self.vocabulary))\n",
    "    \n",
    "    def backprop(self, context, target):\n",
    "        \"\"\" Computes backpropagation of errors to weight matrices,\n",
    "        using stochastic gradient descent \"\"\"\n",
    "\n",
    "        for word in self.vocabulary:\n",
    "\n",
    "            h = self.WI[self.vocabulary[context]] # context word weight vector\n",
    "            P_word_context = self.softmax_regression(word, h) # posterior probability P(word | context)\n",
    "            \n",
    "            if word == target:\n",
    "                t = 1\n",
    "                #print \"P(target|context)\", P_word_context\n",
    "            else:\n",
    "                t = 0\n",
    "\n",
    "            err = t - P_word_context # error\n",
    "\n",
    "            # weight update using stochastic gradient descent\n",
    "            self.WO.T[self.vocabulary[word]] -= self.learning_rate * err * h\n",
    "            # update brings word vector closer in the feature space if word == target, and push them apart otherwise.\n",
    "\n",
    "        # update only weights for input word\n",
    "        self.WI[self.vocabulary[context]] -= self.learning_rate * self.WO.sum(axis = 1) \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\" trains text and returns trained word matrix\n",
    "        and ordered dictionary of vocabulary\"\"\"\n",
    "\n",
    "        bow = self.sentences2bow()\n",
    "        # visualize bag-of-word sentence conversion\n",
    "        # print bow\n",
    "        self.random_init()\n",
    "        \n",
    "        # train text by predicting next word in the sentence\n",
    "        for sentence in self.sentences:\n",
    "            prev_word = None\n",
    "            for word in sentence.split():\n",
    "                if prev_word != None:\n",
    "                    target = word\n",
    "                    context = prev_word\n",
    "                    self.backprop(context, target)\n",
    "                prev_word = word\n",
    "\n",
    "        return self.WI, OrderedDict(sorted(self.vocabulary.items(), key = lambda t: t[1]))\n",
    "\n",
    "    def graph_vector_space(self):\n",
    "        \"\"\" 3D Scatter plot of first 3 word features with Plotly\"\"\"\n",
    "        vocab = OrderedDict(sorted(self.vocabulary.items(), key = lambda t: t[1]))\n",
    "\n",
    "        trace1 = Scatter3d(\n",
    "            x = self.WI.T[0],\n",
    "            y = self.WI.T[1],\n",
    "            z = self.WI.T[2],\n",
    "            mode ='markers+text',\n",
    "            text = vocab.keys(),\n",
    "            marker = Marker(\n",
    "                size = 8,\n",
    "                line = Line(\n",
    "                    color = 'rgba(217, 217, 217, 0.14)',\n",
    "                    width = 0.5\n",
    "                ),\n",
    "                opacity = 0.8\n",
    "            )\n",
    "        )\n",
    "        data = Data([trace1])\n",
    "        layout = Layout(\n",
    "            margin = Margin(\n",
    "                l = 0,\n",
    "                r = 0,\n",
    "                b = 0,\n",
    "                t = 0\n",
    "            )\n",
    "        )\n",
    "        fig = Figure(data = data, layout = layout)\n",
    "        return py.iplot(fig)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sentences = ['<s> the prince loves skateboarding in the park </s>', \n",
    "                 '<s> the princess loves the prince but the princess hates skateboarding </s>',\n",
    "                 '<s> skateboarding in the park is popular </s>',\n",
    "                 '<s> the prince is popular but the prince hates attention </s>',\n",
    "                 '<s> the princess loves attention but the princess hates the park </s>']\n",
    "\n",
    "    model = Word2Vec_1WordContext(sentences, learning_rate = 1.0)\n",
    "    WI, vocab = model.train()\n",
    "    print WI, vocab\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
